{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f12a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb666603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "df = pd.read_csv('../data/sgjobdata.csv.xz')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert category in json format to a string format\n",
    "\n",
    "def categories_json_to_string(df, col=\"categories\", sep=\"; \"):\n",
    "    out = df.copy()\n",
    "\n",
    "    def parse_and_join(x):\n",
    "        if pd.isna(x):\n",
    "            return None\n",
    "        if isinstance(x, str):\n",
    "            try:\n",
    "                data = json.loads(x)\n",
    "            except json.JSONDecodeError:\n",
    "                return None\n",
    "        else:\n",
    "            data = x\n",
    "\n",
    "        if isinstance(data, list):\n",
    "            return sep.join(\n",
    "                d.get(\"category\") for d in data\n",
    "                if isinstance(d, dict) and \"category\" in d\n",
    "            )\n",
    "\n",
    "        return None\n",
    "\n",
    "    out[col] = out[col].map(parse_and_join)\n",
    "    return out\n",
    "\n",
    "###\n",
    "def clean_title_light(t: str) -> str:\n",
    "    t = (t or \"\").strip().lower()\n",
    "    # Keep bracket content, seniority, domain hints; remove only separators/noisy symbols\n",
    "    t = re.sub(r\"[_#|]+\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf65688",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df_categories_str = categories_json_to_string(df)\n",
    "df_categories_str = df_categories_str.dropna(subset=[\"title\"])\n",
    "df_categories_str[\"title_analysis\"] = df_categories_str[\"title\"] + \"-\" + df_categories_str[\"categories\"]\n",
    "df_categories_str[\"title_analysis\"] = df_categories_str[\"title_analysis\"].map(clean_title_light)\n",
    "df_categories_str.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fcf42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_categories(df, col=\"categories\"):\n",
    "    out = df.copy()\n",
    "\n",
    "    out[col] = (\n",
    "        out[col]\n",
    "        .dropna()\n",
    "        .map(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "    )\n",
    "\n",
    "    out = out.explode(col, ignore_index=True)\n",
    "\n",
    "    out[\"category_id\"] = out[col].map(lambda d: d.get(\"id\") if isinstance(d, dict) else None)\n",
    "    out[\"category\"]    = out[col].map(lambda d: d.get(\"category\") if isinstance(d, dict) else None)\n",
    "\n",
    "    return out.drop(columns=[col])\n",
    "\n",
    "df_exploded = explode_categories(df)\n",
    "df_exploded = df_exploded.dropna(subset=[\"category_id\", \"category\"])\n",
    "df_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e0082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if company name contains 'kpmg', show list of such rows\n",
    "df_exploded[df_exploded['postedCompany_name'].str.contains('kpmg', case=False, na=False)].sort_values('metadata_originalPostingDate', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eaf001",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded[df_exploded['category'] == 'Accounting / Auditing / Taxation'][['category','postedCompany_name','metadata_jobPostId','numberOfVacancies']] \\\n",
    "    .groupby('postedCompany_name').numberOfVacancies.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58109dc2",
   "metadata": {},
   "source": [
    "## CLean Job Title data & use it to predict top 3 skillsets using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8300e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    pipeline,\n",
    ")\n",
    "from transformers.pipelines import PIPELINE_REGISTRY\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# A) Cleaning (minimal, preserve nuance)\n",
    "# ============================================================\n",
    "def clean_title_light(t: str) -> str:\n",
    "    t = (t or \"\").strip().lower()\n",
    "    # Keep bracket content, seniority, domain hints; remove only separators/noisy symbols\n",
    "    t = re.sub(r\"[_#|]+\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# B) Embedding + scalable \"loose\" clustering\n",
    "# ============================================================\n",
    "def embed_unique_titles(unique_titles: list[str],\n",
    "                        model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                        device: str = \"cpu\",\n",
    "                        batch_size: int = 256) -> np.ndarray:\n",
    "    embedder = SentenceTransformer(model_name, device=device)\n",
    "    emb = embedder.encode(\n",
    "        unique_titles,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    return emb\n",
    "\n",
    "\n",
    "def cluster_embeddings_loose(embeddings: np.ndarray,\n",
    "                             n_clusters: int = 800,\n",
    "                             batch_size: int = 8192,\n",
    "                             random_state: int = 42) -> np.ndarray:\n",
    "    km = MiniBatchKMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        batch_size=batch_size,\n",
    "        random_state=random_state,\n",
    "        n_init=\"auto\"\n",
    "    )\n",
    "    return km.fit_predict(embeddings)\n",
    "\n",
    "\n",
    "def canonical_title_per_cluster(u: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    u must contain: job_title_cleaned, cluster_id, count\n",
    "    Returns canonical mapping: cluster_id -> clustered_job_title (most frequent in cluster)\n",
    "    \"\"\"\n",
    "    canon = (\n",
    "        u.sort_values([\"cluster_id\", \"count\"], ascending=[True, False])\n",
    "         .groupby(\"cluster_id\", as_index=False)\n",
    "         .head(1)[[\"cluster_id\", \"job_title_cleaned\"]]\n",
    "         .rename(columns={\"job_title_cleaned\": \"clustered_job_title\"})\n",
    "    )\n",
    "    return canon\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# C) LLM backend (supports seq2seq and causal, robust)\n",
    "# ============================================================\n",
    "def _supports_pipeline(task_name: str) -> bool:\n",
    "    try:\n",
    "        PIPELINE_REGISTRY.check_task(task_name)\n",
    "        return True\n",
    "    except KeyError:\n",
    "        return False\n",
    "\n",
    "\n",
    "class SkillInferencer:\n",
    "    \"\"\"\n",
    "    A robust local inferencer that works on Mac CPU:\n",
    "    - Supports causal instruct models (text-generation)\n",
    "    - Supports seq2seq models (Flan-T5) using pipeline if available,\n",
    "      otherwise uses direct model.generate() to avoid pipeline task errors.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 hf_model_name: str,\n",
    "                 backend: str = \"auto\",  # \"auto\" | \"seq2seq\" | \"causal\"\n",
    "                 device: str | None = None):\n",
    "        self.hf_model_name = hf_model_name\n",
    "        self.backend = backend\n",
    "        self.device = device  # \"cpu\" or \"cuda\" (we assume cpu on M1)\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.gen_pipe = None\n",
    "        self._init_model()\n",
    "\n",
    "    def _init_model(self):\n",
    "        cfg = AutoConfig.from_pretrained(self.hf_model_name)\n",
    "        model_type = getattr(cfg, \"model_type\", \"\").lower()\n",
    "\n",
    "        if self.backend == \"auto\":\n",
    "            # Heuristic: t5/mt5/bart/pegasus => seq2seq; else causal\n",
    "            if model_type in {\"t5\", \"mt5\", \"bart\", \"mbart\", \"pegasus\"}:\n",
    "                self.backend = \"seq2seq\"\n",
    "            else:\n",
    "                self.backend = \"causal\"\n",
    "\n",
    "        if self.device is None:\n",
    "            # M1: CPU\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.hf_model_name)\n",
    "\n",
    "        if self.backend == \"seq2seq\":\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(self.hf_model_name)\n",
    "            self.model.to(self.device)\n",
    "            # Use pipeline only if task exists; otherwise we use generate() directly\n",
    "            if _supports_pipeline(\"text2text-generation\"):\n",
    "                pipe_device = 0 if self.device == \"cuda\" else -1\n",
    "                self.gen_pipe = pipeline(\n",
    "                    \"text2text-generation\",\n",
    "                    model=self.model,\n",
    "                    tokenizer=self.tokenizer,\n",
    "                    device=pipe_device\n",
    "                )\n",
    "            else:\n",
    "                self.gen_pipe = None\n",
    "\n",
    "        elif self.backend == \"causal\":\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.hf_model_name,\n",
    "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n",
    "            )\n",
    "            self.model.to(self.device)\n",
    "            pipe_device = 0 if self.device == \"cuda\" else -1\n",
    "            self.gen_pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                device=pipe_device\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"backend must be 'auto', 'seq2seq', or 'causal'\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _prompt(title: str) -> str:\n",
    "        return (\n",
    "            \"Return JSON only.\\n\"\n",
    "            \"Task: Infer the top 5 core skills required for the job title.\\n\"\n",
    "            f\"Job title: {title}\\n\"\n",
    "            \"Rules:\\n\"\n",
    "            \"- Exactly 5 skills\\n\"\n",
    "            \"- Short noun phrases (1–3 words)\\n\"\n",
    "            \"- No explanation\\n\"\n",
    "            'JSON schema: {\"skills\":[\"skill1\",\"skill2\",\"skill3\",\"skill4\",\"skill5\"]}'\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_json(text: str) -> dict:\n",
    "        # First try: extract first {...}\n",
    "        s = text.find(\"{\")\n",
    "        e = text.rfind(\"}\") + 1\n",
    "        if s != -1 and e != -1 and e > s:\n",
    "            candidate = text[s:e]\n",
    "            try:\n",
    "                obj = json.loads(candidate)\n",
    "                skills = obj.get(\"skills\", [])\n",
    "                if isinstance(skills, list):\n",
    "                    skills = [re.sub(r\"\\s+\", \" \", str(x).strip()) for x in skills if str(x).strip()]\n",
    "                    skills = skills[:3]\n",
    "                    while len(skills) < 3:\n",
    "                        skills.append(\"unknown\")\n",
    "                    return {\"skills\": skills}\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Fallback: pull quoted strings\n",
    "        items = [x for x in re.findall(r'\"([^\"]+)\"', text) if x.lower() != \"skills\"]\n",
    "        items = [re.sub(r\"\\s+\", \" \", x.strip()) for x in items if x.strip()]\n",
    "        items = items[:3]\n",
    "        while len(items) < 3:\n",
    "            items.append(\"unknown\")\n",
    "        return {\"skills\": items}\n",
    "\n",
    "    def infer(self, title: str, max_new_tokens: int = 80) -> dict:\n",
    "        prompt = self._prompt(title)\n",
    "\n",
    "        if self.backend == \"causal\":\n",
    "            out = self.gen_pipe(\n",
    "                prompt,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                return_full_text=False\n",
    "            )[0][\"generated_text\"]\n",
    "            return self._parse_json(out)\n",
    "\n",
    "        # seq2seq\n",
    "        if self.gen_pipe is not None:\n",
    "            out = self.gen_pipe(prompt, max_new_tokens=max_new_tokens, do_sample=False)[0][\"generated_text\"]\n",
    "            return self._parse_json(out)\n",
    "\n",
    "        # seq2seq fallback: direct generate() (avoids pipeline task errors)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            gen_ids = self.model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        out = self.tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "        return self._parse_json(out)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# D) End-to-end scalable function (1M rows safe)\n",
    "# ============================================================\n",
    "def build_title_skill_table(\n",
    "    df: pd.DataFrame,\n",
    "    raw_col: str,\n",
    "    # embedding\n",
    "    embed_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    embed_device: str = \"cpu\",\n",
    "    embed_batch_size: int = 256,\n",
    "    # clustering\n",
    "    n_clusters: int = 800,\n",
    "    km_batch_size: int = 8192,\n",
    "    # HF model\n",
    "    hf_model_name: str = \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    llm_backend: str = \"auto\",         # \"auto\"|\"causal\"|\"seq2seq\"\n",
    "    skills_per: str = \"cluster\",       # \"cluster\" recommended\n",
    "    # performance\n",
    "    max_new_tokens: int = 60,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Output columns (exactly):\n",
    "    a. job_title_raw (pre-clean)\n",
    "    b. job_title_cleaned\n",
    "    c. clustered_job_title\n",
    "    d. top_3_skills_json (dict)\n",
    "    \"\"\"\n",
    "    if raw_col not in df.columns:\n",
    "        raise ValueError(f\"Missing required column: {raw_col}\")\n",
    "    if skills_per not in {\"cluster\", \"unique_title\"}:\n",
    "        raise ValueError(\"skills_per must be 'cluster' or 'unique_title'\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 1) Clean\n",
    "    df_work = df.copy()\n",
    "    df_work[\"job_title_cleaned\"] = df_work[raw_col].map(clean_title_light)\n",
    "\n",
    "    # 2) Unique + freq (for canonical titles)\n",
    "    u = (\n",
    "        df_work.groupby(\"job_title_cleaned\")\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    unique_titles = u[\"job_title_cleaned\"].tolist()\n",
    "\n",
    "    print(f\"Rows: {len(df_work):,}\")\n",
    "    print(f\"Unique cleaned titles: {len(unique_titles):,}\")\n",
    "\n",
    "    # 3) Embed uniques\n",
    "    print(\"\\n[1/4] Embedding unique titles...\")\n",
    "    emb = embed_unique_titles(unique_titles, embed_model_name, embed_device, embed_batch_size)\n",
    "    print(\"Embeddings:\", emb.shape, emb.dtype)\n",
    "\n",
    "    # 4) Cluster uniques\n",
    "    print(\"\\n[2/4] Clustering (MiniBatchKMeans)...\")\n",
    "    u[\"cluster_id\"] = cluster_embeddings_loose(emb, n_clusters=n_clusters, batch_size=km_batch_size)\n",
    "\n",
    "    # 5) Canonical per cluster\n",
    "    canon = canonical_title_per_cluster(u)\n",
    "    u = u.merge(canon, on=\"cluster_id\", how=\"left\")\n",
    "\n",
    "    # 6) Load local LLM backend\n",
    "    print(\"\\n[3/4] Loading local HF model...\")\n",
    "    inferencer = SkillInferencer(hf_model_name=hf_model_name, backend=llm_backend, device=None)\n",
    "    print(\"LLM backend selected:\", inferencer.backend)\n",
    "\n",
    "    # 7) Decide inference targets\n",
    "    if skills_per == \"cluster\":\n",
    "        targets = canon[\"clustered_job_title\"].drop_duplicates().tolist()\n",
    "        key_col = \"clustered_job_title\"\n",
    "        print(f\"Skill inference targets (clusters): {len(targets):,}\")\n",
    "    else:\n",
    "        targets = unique_titles\n",
    "        key_col = \"job_title_cleaned\"\n",
    "        print(f\"Skill inference targets (unique titles): {len(targets):,}\")\n",
    "\n",
    "    # 8) Infer skills once per target (cache)\n",
    "    print(\"\\n[4/4] Inferring top-3 skills (cached)...\")\n",
    "    cache = {}\n",
    "    for t in tqdm(targets):\n",
    "        if t not in cache:\n",
    "            cache[t] = inferencer.infer(t, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    u[\"top_3_skills_json\"] = u[key_col].map(cache)\n",
    "\n",
    "    # 9) Map back to 1M rows\n",
    "    df_out = df_work.merge(\n",
    "        u[[\"job_title_cleaned\", \"cluster_id\", \"clustered_job_title\", \"top_3_skills_json\"]],\n",
    "        on=\"job_title_cleaned\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    df_out = df_out[[raw_col, \"job_title_cleaned\", \"clustered_job_title\", \"top_3_skills_json\"]].rename(\n",
    "        columns={raw_col: \"job_title_raw\"}\n",
    "    )\n",
    "\n",
    "    print(f\"\\nDone. Total seconds: {round(time.time() - t0, 1)}\")\n",
    "    return df_out\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# E) Run a fail-fast small test first\n",
    "# ============================================================\n",
    "def sanity_test(df: pd.DataFrame, raw_col: str) -> pd.DataFrame:\n",
    "    small = df[[raw_col]].dropna().head(50).copy()\n",
    "    return build_title_skill_table(\n",
    "        df=small,\n",
    "        raw_col=raw_col,\n",
    "        n_clusters=10,\n",
    "        skills_per=\"cluster\",\n",
    "        hf_model_name=\"Qwen/Qwen2.5-0.5B-Instruct\",  # fast\n",
    "        llm_backend=\"causal\",\n",
    "        max_new_tokens=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "519c2747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 211/300 [1:01:55<1:30:27, 60.99s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 71%|███████   | 212/300 [1:02:09<1:08:39, 46.81s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 71%|███████   | 213/300 [1:02:22<53:08, 36.65s/it]  Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 71%|███████▏  | 214/300 [1:02:36<42:38, 29.75s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 72%|███████▏  | 215/300 [1:02:49<35:19, 24.93s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 72%|███████▏  | 216/300 [1:03:03<30:13, 21.59s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 72%|███████▏  | 217/300 [1:03:17<26:36, 19.24s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 73%|███████▎  | 218/300 [1:03:28<22:59, 16.83s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 73%|███████▎  | 219/300 [1:03:42<21:26, 15.89s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 73%|███████▎  | 220/300 [1:03:55<20:13, 15.17s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 74%|███████▎  | 221/300 [1:04:09<19:26, 14.76s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 74%|███████▍  | 222/300 [1:04:23<18:47, 14.45s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 74%|███████▍  | 223/300 [1:04:36<18:14, 14.21s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 75%|███████▍  | 224/300 [1:04:50<17:50, 14.08s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 75%|███████▌  | 225/300 [1:05:04<17:27, 13.97s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 75%|███████▌  | 226/300 [1:05:17<16:57, 13.74s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 76%|███████▌  | 227/300 [1:05:31<16:42, 13.73s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 76%|███████▌  | 228/300 [1:05:44<16:12, 13.51s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 76%|███████▋  | 229/300 [1:05:57<15:57, 13.49s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 77%|███████▋  | 230/300 [1:06:11<15:52, 13.60s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 77%|███████▋  | 231/300 [1:06:22<14:53, 12.95s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 77%|███████▋  | 232/300 [1:06:36<14:55, 13.18s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 78%|███████▊  | 233/300 [1:06:49<14:39, 13.13s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 78%|███████▊  | 234/300 [1:07:03<14:38, 13.30s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 78%|███████▊  | 235/300 [1:07:17<14:34, 13.45s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 79%|███████▊  | 236/300 [1:07:31<14:27, 13.55s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 79%|███████▉  | 237/300 [1:07:44<14:18, 13.63s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 79%|███████▉  | 238/300 [1:07:58<14:05, 13.64s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 80%|███████▉  | 239/300 [1:08:10<13:19, 13.11s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 80%|████████  | 240/300 [1:08:23<13:03, 13.05s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 80%|████████  | 241/300 [1:08:36<13:01, 13.24s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 81%|████████  | 242/300 [1:08:50<12:54, 13.36s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 81%|████████  | 243/300 [1:09:02<12:24, 13.06s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 81%|████████▏ | 244/300 [1:09:16<12:22, 13.26s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 82%|████████▏ | 245/300 [1:09:30<12:20, 13.46s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 82%|████████▏ | 246/300 [1:09:44<12:12, 13.57s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 82%|████████▏ | 247/300 [1:09:58<11:59, 13.57s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 83%|████████▎ | 248/300 [1:10:11<11:48, 13.62s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 83%|████████▎ | 249/300 [1:10:25<11:35, 13.65s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 83%|████████▎ | 250/300 [1:10:37<10:56, 13.12s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 84%|████████▎ | 251/300 [1:10:49<10:26, 12.78s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 84%|████████▍ | 252/300 [1:11:01<10:03, 12.57s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 84%|████████▍ | 253/300 [1:11:13<09:47, 12.49s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 85%|████████▍ | 254/300 [1:11:27<09:50, 12.84s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 85%|████████▌ | 255/300 [1:11:40<09:40, 12.90s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 85%|████████▌ | 256/300 [1:11:53<09:30, 12.96s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 86%|████████▌ | 257/300 [1:12:05<09:09, 12.77s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 86%|████████▌ | 258/300 [1:12:16<08:28, 12.10s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 86%|████████▋ | 259/300 [1:12:29<08:22, 12.27s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 87%|████████▋ | 260/300 [1:12:42<08:21, 12.54s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 87%|████████▋ | 261/300 [1:12:56<08:26, 12.98s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 87%|████████▋ | 262/300 [1:13:10<08:22, 13.22s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 88%|████████▊ | 263/300 [1:13:23<08:14, 13.37s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 88%|████████▊ | 264/300 [1:13:36<07:49, 13.05s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 88%|████████▊ | 265/300 [1:13:48<07:32, 12.94s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 89%|████████▊ | 266/300 [1:14:01<07:12, 12.73s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 89%|████████▉ | 267/300 [1:14:13<07:01, 12.78s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 89%|████████▉ | 268/300 [1:14:27<06:58, 13.08s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 90%|████████▉ | 269/300 [1:14:41<06:50, 13.24s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 90%|█████████ | 270/300 [1:14:55<06:44, 13.48s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 90%|█████████ | 271/300 [1:15:06<06:15, 12.93s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 91%|█████████ | 272/300 [1:15:20<06:08, 13.16s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 91%|█████████ | 273/300 [1:15:34<05:59, 13.33s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 91%|█████████▏| 274/300 [1:15:48<05:49, 13.43s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 92%|█████████▏| 275/300 [1:15:59<05:18, 12.74s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 92%|█████████▏| 276/300 [1:16:12<05:08, 12.86s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 92%|█████████▏| 277/300 [1:16:25<04:54, 12.82s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 93%|█████████▎| 278/300 [1:16:38<04:47, 13.08s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 93%|█████████▎| 279/300 [1:16:52<04:38, 13.28s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 93%|█████████▎| 280/300 [1:17:06<04:28, 13.45s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 94%|█████████▎| 281/300 [1:17:20<04:17, 13.53s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 94%|█████████▍| 282/300 [1:17:33<04:04, 13.59s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 94%|█████████▍| 283/300 [1:17:47<03:51, 13.64s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 95%|█████████▍| 284/300 [1:18:00<03:33, 13.33s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 95%|█████████▌| 285/300 [1:18:14<03:22, 13.48s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 95%|█████████▌| 286/300 [1:18:27<03:09, 13.56s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 96%|█████████▌| 287/300 [1:18:39<02:50, 13.15s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 96%|█████████▌| 288/300 [1:18:53<02:39, 13.33s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 96%|█████████▋| 289/300 [1:19:01<02:09, 11.74s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 97%|█████████▋| 290/300 [1:19:15<02:03, 12.36s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 97%|█████████▋| 291/300 [1:19:29<01:55, 12.78s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 97%|█████████▋| 292/300 [1:19:41<01:41, 12.64s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 98%|█████████▊| 293/300 [1:19:54<01:29, 12.75s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 98%|█████████▊| 294/300 [1:20:08<01:18, 13.05s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 98%|█████████▊| 295/300 [1:20:18<01:01, 12.23s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 99%|█████████▊| 296/300 [1:20:32<00:50, 12.71s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 99%|█████████▉| 297/300 [1:20:46<00:38, 13.00s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 99%|█████████▉| 298/300 [1:20:58<00:25, 12.79s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "100%|█████████▉| 299/300 [1:21:11<00:12, 12.75s/it]Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "100%|██████████| 300/300 [1:21:24<00:00, 16.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. Total seconds: 5195.5\n"
     ]
    }
   ],
   "source": [
    "_ = sanity_test(df_categories_str, raw_col=\"title_analysis\")\n",
    "\n",
    "df_out = build_title_skill_table(\n",
    "    df=df_categories_str,\n",
    "    raw_col=\"title_analysis\",\n",
    "    n_clusters=300,                 # 50 is too coarse for real job titles\n",
    "    skills_per=\"cluster\",\n",
    "    hf_model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    llm_backend=\"causal\",\n",
    "    max_new_tokens=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e694aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_parquet('sgjobdata_titleskills_v2_5skills.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a0cd99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
